<!DOCTYPE html><html lang="cn"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>深度强化学习落地指南 05 | Sparkle</title><script src="https://cdn.bootcss.com/valine/1.4.4/Valine.min.js"></script><link rel="stylesheet" href="/css/arknights.css"><link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/highlight.js/10.1.2/styles/atom-one-dark-reasonable.min.css"><style>@font-face {
 font-family: BenderLight;
 src: local('Bender'), url("/font/BenderLight.ttf");
}
@font-face {
 font-family: 'JetBrains Mono';
 src: local('JetBrains Mono'), url('/font/JetBrainsMono-Regular.woff2') format('woff2');
}</style><meta name="generator" content="Hexo 5.4.0"></head><body><header><nav><a href="/">Home</a><a href="/archives/">Archives</a></nav></header><main><article><div id="post-bg"><div id="post-title"><h1>深度强化学习落地指南 05</h1><hr></div><div id="post-content"><h1 id="第五章-算法选择"><a href="#第五章-算法选择" class="headerlink" title="第五章 算法选择"></a>第五章 算法选择</h1><h2 id="拿来主义-amp-改良主义"><a href="#拿来主义-amp-改良主义" class="headerlink" title="拿来主义 &amp; 改良主义"></a>拿来主义 &amp; 改良主义</h2><p>尽管深度强化学习算法已经取得长足进步，但尚未在理论层面取得质的突破，而只是在传统强化学习理论基础上引入深度神经网络，并做了一系列适配和增量式改进工作。</p>
<p>总体上，DRL 算法沿着 Model-Based 和 Model-Free 两大分支发展。</p>
<h3 id="Model-Based"><a href="#Model-Based" class="headerlink" title="Model-Based"></a>Model-Based</h3><p>Model-Based 算法利用已知环境模型/对未知环境模型进行显式建模，并与<strong>前向搜索 (Look Ahead Search)</strong> 和<strong>轨迹优化 (Trajectory Optimization)</strong> 等规划算法结合达到提升数据效率的目的。但未在实践中得到广泛应用，因为现实任务的环境模型通常十分复杂，导致模型学习的难度很高<a href="#refer-anchor"><sup>[1,2]</sup></a>，并且建模误差也会对策略造成负面影响。</p>
<h3 id="Model-Free"><a href="#Model-Free" class="headerlink" title="Model-Free"></a>Model-Free</h3><p>Model-Free 算法可以解构为“基本原理——探索方式——样本管理——梯度计算”的四元核心组件。<br>按照基本原理可以分为：Value-Based &amp; Policy-Based；On-Policy &amp; Off-Policy。</p>
<ul>
<li>Value-Based：算法直接学习状态-动作组合的值估计；</li>
<li>Policy-Based：算法具有独立策略；</li>
<li>Actor-Critic：同时具备独立策略和值估计函数的算法；</li>
<li>On-Policy：采样策略和待优化策略相同或差异很小；</li>
<li>Off-Policy：采样策略和待优化策略不同。</li>
</ul>
<p>DQN<a href="#refer-anchor"><sup>[3]</sup></a>、DDPG<a href="#refer-anchor"><sup>[4]</sup></a>、A3C<a href="#refer-anchor"><sup>[5]</sup></a> 作为这两种彼此交织的划分体系下的经典算法框架，构成了 DRL 研究中的重要节点，后续提出的大部分算法基本都立足于这三种框架，针对其核心组件所进行的迭代优化或者拆分重组。</p>
<p>四元核心组件：</p>
<ul>
<li>基本原理方面进展缓慢，但却是 DRL 算法将来大规模推广的关键所在。</li>
<li>探索方式的改进使算法更充分探索环境，更好地平衡探索和利用，从而有机会学习到更好的策略。</li>
<li>样本管理的改进有助于提升算法的样本效率，加快收敛速度，提高算法实用性。</li>
<li>梯度计算的改进致力于使每一次梯度更新都更稳定、无偏和高效。</li>
</ul>
<p>总体而言，深度强化学习算法正朝着通用化和高效化的方向发展。</p>
<h3 id="一筛、二比、三改良"><a href="#一筛、二比、三改良" class="headerlink" title="一筛、二比、三改良"></a>一筛、二比、三改良</h3><p>粗略来看，根据问题定义、动作空间类型、采样成本和可用计算资源等因素的不同，确实存在一些关于不同类型 DRL 算法适用性方面的结论。</p>
<ul>
<li>DQN 及其变体一般只适用于离散动作空间；</li>
<li>DDPG 及其变体只适合连续动作空间；</li>
<li>A3C 及 SAC 等则支持离散和连续动作空间。</li>
<li>随机性策略通常比确定性策略有更好的训练稳定性。</li>
</ul>
<p>对于机器人等涉及硬件的应用或采样成本较高的任务，能够重复利用历史数据的 Off-Policy 算法比 On-Policy 算法更有优势<a href="#refer-anchor"><sup>[7]</sup></a>。</p>
<p>在多智能体强化学习任务中，多个交互的智能体互相构成对方环境的一部分，并随着各自策略的迭代导致这些环境模型发生变化，从而导致基于这些模型构建的知识/技能失效，这种现象称为<strong>环境不稳定性 (Environment Nonstationarity)<strong>。因此除非</strong>经验回放缓存 (Replay Buffer)</strong> 中的数据更新得足够快，否则 Off-Policy 算法反而可能引入偏差<a href="#refer-anchor"><sup>[8]</sup></a>。</p>
<p>由于利用贝尔曼公式 Bootstrap 特性得值迭代方法是有偏得，On-Policy 算法在训练稳定性方面要好于 Off-Policy 算法。然而为了尽可能获取关于值函数的无偏估计，On-Policy 算法往往需要利用多个环境并行采集足够多的样本，这对硬件有所要求，而 Off-Policy 则不必，虽然也能够从并行采样中受益<a href="#refer-anchor"><sup>[5]</sup></a>。</p>
<p>下表总结了 Model-Free DRL 算法适用性的一般结论：</p>
<p>||||</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><div id="refer-anchor"></div>
1. 
2.
3.
4.
5.
6.
7.
8.<div id="paginator"></div></div><div id="post-footer"><hr><a href="/post/d04bd5bc/">← Prev 操作系统</a><span style="color: #fe2"> | </span><a href="/post/e255a10a/">计算机网络 Next →</a><hr></div><div id="bottom-btn"><a id="to-index" href="#post-index" title="index">≡</a><a id="to-top" href="#post-title" title="to top">∧</a></div><div id="Valine"></div><script>new Valine({
 el: '#Valine'
 , appId: ''
 , appKey: ''
 , placeholder: '此条评论委托企鹅物流发送'
})</script></div></article><aside><div id="about"><a href="/" id="logo"><img src="https://cdn.jsdelivr.net/gh/Sparklesyo/Collections/img/FNnex.png" alt="Logo"></a><h1 id="Dr"><a href="/"> Dr.斯帕蔻</a></h1><section id="total"><a id="total-archives" href="/archives"><span class="total-title">Archives Total:</span><span class="total-number">25</span></a><div id="total-tags"><span class="total-title">Tags:</span><span class="total-number">17</span></div><div id="total-categories"><span class="total-title">Categories:</span><span class="total-number">14</span></div></section></div><div id="aside-block"><h1>INDEX</h1><div id="post-index"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%AE%97%E6%B3%95%E9%80%89%E6%8B%A9"><span class="toc-number">1.</span> <span class="toc-text">第五章 算法选择</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8B%BF%E6%9D%A5%E4%B8%BB%E4%B9%89-amp-%E6%94%B9%E8%89%AF%E4%B8%BB%E4%B9%89"><span class="toc-number">1.1.</span> <span class="toc-text">拿来主义 &amp; 改良主义</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Model-Based"><span class="toc-number">1.1.1.</span> <span class="toc-text">Model-Based</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Model-Free"><span class="toc-number">1.1.2.</span> <span class="toc-text">Model-Free</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E7%AD%9B%E3%80%81%E4%BA%8C%E6%AF%94%E3%80%81%E4%B8%89%E6%94%B9%E8%89%AF"><span class="toc-number">1.1.3.</span> <span class="toc-text">一筛、二比、三改良</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">1.2.</span> <span class="toc-text">参考文献</span></a></li></ol></li></ol></div></div><footer><nobr><span class="text-title">©</span><span class="text-content">2021</span></nobr><wbr><nobr><span class="text-title">ICP</span><span class="text-content"></span></nobr><wbr><wbr><nobr>published with&nbsp;<a target="_blank" rel="noopener" href="http://hexo.io">Hexo&nbsp;</a></nobr><wbr><nobr>Theme&nbsp;<a target="_blank" rel="noopener" href="https://github.com/Yue-plus/hexo-theme-arknights">Arknight&nbsp;</a></nobr><wbr><nobr>by&nbsp;<a target="_blank" rel="noopener" href="https://github.com/Yue-plus">Yue_plus</a></nobr></footer></aside></main><canvas id="canvas-dust"></canvas><script src="/js/arknights.js"></script><script src="https://cdn.bootcdn.net/ajax/libs/highlight.js/10.1.2/highlight.min.js"></script></body></html>